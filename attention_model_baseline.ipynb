{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LouisStefanuto/Detection-of-the-PIK3CA-mutation-in-breast-cancer/blob/dirty-do-not-trust/attention_model_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOUh2HptHjx"
      },
      "source": [
        "Before starting, you will need to install some packages to reproduce the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:49.282534Z",
          "start_time": "2022-10-24T07:22:35.359325Z"
        },
        "id": "LYHxK0njtHjz",
        "outputId": "b2c6ecd9-494e-4e4c-bab9-5d278b09e780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:50.448803Z",
          "start_time": "2022-10-24T07:22:49.285830Z"
        },
        "id": "L4-Z4UoCtHj0"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KxW2WuijtHj2",
        "outputId": "6ab94ff0-0df4-4fe7-ec0a-6a54a3e012e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/challenge_data_ens_small/'\n",
            "/content\n",
            "Archive:  /content/drive/MyDrive/challenge_ens_2023_small/moco_features.zip\n",
            "replace /content/test_input/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# import data\n",
        "PATH_COLAB = '/content/drive/MyDrive/challenge_ens_2023_small/moco_features.zip'\n",
        "PATH_DEVICE = '..'\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    logging.info('Working on Colab.')\n",
        "    \n",
        "    # connect your drive to the session\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    %cd /content/drive/MyDrive/challenge_data_ens_small/\n",
        "\n",
        "    # unzip data into the colab session\n",
        "    ! unzip $PATH_COLAB -d /content\n",
        "    logging.info('Data unziped in your Drive.')\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "    %cp -R drive/MyDrive/challenge_ens_2023_small/supplementary_data/ .\n",
        "    %cp drive/MyDrive/challenge_ens_2023_small/train_output.csv .\n",
        "\n",
        "\n",
        "except:\n",
        "    logging.info('Working on your device.')\n",
        "    \n",
        "    data_exists = os.path.exists(PATH_DEVICE + '/train_input') and os.path.exists(PATH_DEVICE + '/test_input') and os.path.exists(PATH_DEVICE + '/train_output.csv')\n",
        "    \n",
        "    if data_exists:\n",
        "        logging.info(f\"Dataset found on device at : '{PATH_DEVICE}.'\") \n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Data folder not found at '{PATH_DEVICE}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZwNiwkdUtHj4",
        "outputId": "0a86995a-364f-459e-85f6-5b9a61b1acda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g561bAvWtHj4"
      },
      "source": [
        "# Data architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-20T15:29:41.479814Z",
          "start_time": "2022-10-20T15:29:41.472869Z"
        },
        "id": "HTGcDg_OtHj5"
      },
      "source": [
        "After downloading or unzipping the downloaded files, your data tree must have the following architecture in order to properly run the notebook:\n",
        "```\n",
        "your_data_dir/\n",
        "├── train_output.csv\n",
        "├── train_input/\n",
        "│   ├── images/\n",
        "│       ├── ID_001/\n",
        "│           ├── ID_001_tile_000_17_170_43.jpg\n",
        "...\n",
        "│   └── moco_features/\n",
        "│       ├── ID_001.npy\n",
        "...\n",
        "├── test_input/\n",
        "│   ├── images/\n",
        "│       ├── ID_003/\n",
        "│           ├── ID_003_tile_000_16_114_93.jpg\n",
        "...\n",
        "│   └── moco_features/\n",
        "│       ├── ID_003.npy\n",
        "...\n",
        "├── supplementary_data/\n",
        "│   ├── baseline.ipynb\n",
        "│   ├── test_metadata.csv\n",
        "│   └── train_metadata.csv\n",
        "```\n",
        "\n",
        "For instance, `your_data_dir = /storage/DATA_CHALLENGE_ENS_2022/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYiFEHvttHj6"
      },
      "source": [
        "This notebook aims to reproduce the baseline method on this challenge called `MeanPool`. This method consists in a logistic regression learnt on top of tile-level MoCo V2 features averaged over the slides.\n",
        "\n",
        "For a given slide $s$ with $N_s=1000$ tiles and corresponding MoCo V2 features $\\mathbf{K_s} \\in \\mathbb{R}^{(1000,\\,2048)}$, a slide-level average is performed over the tile axis.\n",
        "\n",
        "For $j=1,...,2048$:\n",
        "\n",
        "$$\\overline{\\mathbf{k_s}}(j) = \\frac{1}{N_s} \\sum_{i=1}^{N_s} \\mathbf{K_s}(i, j) $$\n",
        "\n",
        "Thus, the training input data for MeanPool consists of $S_{\\text{train}}=344$ mean feature vectors $\\mathbf{k_s}$, $s=1,...,S_{\\text{train}}$, where $S_{\\text{train}}$ denotes the number of training samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-20T09:25:58.896288Z",
          "start_time": "2022-10-20T09:25:58.161711Z"
        },
        "id": "HWlYU146tHj6"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:50.479040Z",
          "start_time": "2022-10-24T07:22:50.450662Z"
        },
        "id": "nqplkW7RtHj7",
        "outputId": "c1ef0ca6-6655-4fee-9143-75bd2e186fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data dimensions: (344, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Sample ID Patient ID Center ID  Target\n",
              "0  ID_001.npy      P_001       C_1       0\n",
              "1  ID_002.npy      P_002       C_2       1\n",
              "2  ID_005.npy      P_005       C_5       0\n",
              "3  ID_006.npy      P_006       C_5       0\n",
              "4  ID_007.npy      P_007       C_2       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6dc5a4a7-72de-43db-8abc-3d795d4e3ed9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sample ID</th>\n",
              "      <th>Patient ID</th>\n",
              "      <th>Center ID</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_001.npy</td>\n",
              "      <td>P_001</td>\n",
              "      <td>C_1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_002.npy</td>\n",
              "      <td>P_002</td>\n",
              "      <td>C_2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_005.npy</td>\n",
              "      <td>P_005</td>\n",
              "      <td>C_5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_006.npy</td>\n",
              "      <td>P_006</td>\n",
              "      <td>C_5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_007.npy</td>\n",
              "      <td>P_007</td>\n",
              "      <td>C_2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6dc5a4a7-72de-43db-8abc-3d795d4e3ed9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6dc5a4a7-72de-43db-8abc-3d795d4e3ed9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6dc5a4a7-72de-43db-8abc-3d795d4e3ed9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# put your own path to the data root directory (see example in `Data architecture` section)\n",
        "data_dir = Path(\"./\")\n",
        "\n",
        "# load the training and testing data sets\n",
        "train_features_dir = data_dir / \"train_input\" / \"moco_features\"\n",
        "test_features_dir = data_dir / \"test_input\" / \"moco_features\"\n",
        "df_train = pd.read_csv(data_dir  / \"supplementary_data\" / \"train_metadata.csv\")\n",
        "df_test = pd.read_csv(data_dir  / \"supplementary_data\" / \"test_metadata.csv\")\n",
        "\n",
        "# concatenate y_train and df_train\n",
        "y_train = pd.read_csv(data_dir  / \"train_output.csv\")\n",
        "df_train = df_train.merge(y_train, on=\"Sample ID\")\n",
        "\n",
        "print(f\"Training data dimensions: {df_train.shape}\")  # (344, 4)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv8nKM7YtHj8"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6PqHcGotHj8"
      },
      "source": [
        "We now load the features matrices $\\mathbf{K_s} \\in \\mathbb{R}^{(1000,\\,2048)}$ for $s=1,...,344$ and perform slide-level averaging. This operation should take at most 5 minutes on your laptop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:52.890700Z",
          "start_time": "2022-10-24T07:22:50.480795Z"
        },
        "id": "eWhTx2mstHj9",
        "outputId": "6d241fcd-9af9-4ebb-bcd1-5ffdd1d1403b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344/344 [00:00<00:00, 499944.76it/s]\n"
          ]
        }
      ],
      "source": [
        "#size_train = 50 #len(df_train)\n",
        "#X_train = np.zeros((size_train, 1000, 2048))\n",
        "#y_train = np.zeros((size_train), dtype=int)\n",
        "centers_train = []\n",
        "patients_train = []\n",
        "\n",
        "for i, (sample, label, center, patient) in enumerate(tqdm(\n",
        "    df_train[[\"Sample ID\", \"Target\", \"Center ID\", \"Patient ID\"]].values\n",
        ")):\n",
        "    # load the coordinates and features (1000, 3+2048)\n",
        "    #_features = np.load(train_features_dir / sample)\n",
        "    # get coordinates (zoom level, tile x-coord on the slide, tile y-coord on the slide)\n",
        "    # and the MoCo V2 features\n",
        "    #coordinates, features = _features[:, :3], _features[:, 3:]  # Ks\n",
        "    # slide-level averaging\n",
        "    #X_train[i] = features\n",
        "    #y_train[i] = label\n",
        "    centers_train.append(center)\n",
        "    patients_train.append(patient)\n",
        "    \n",
        "\n",
        "# convert to numpy arrays\n",
        "#X_train = np.array(X_train)\n",
        "#y_train = np.array(y_train)\n",
        "centers_train = np.array(centers_train)\n",
        "patients_train = np.array(patients_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ROC-STAR loss"
      ],
      "metadata": {
        "id": "BkyegTp5LefC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def roc_star_loss( _y_true, y_pred, gamma, _epoch_true, epoch_pred):\n",
        "    \"\"\"\n",
        "    Nearly direct loss function for AUC.\n",
        "    See article,\n",
        "    C. Reiss, \"Roc-star : An objective function for ROC-AUC that actually works.\"\n",
        "    https://github.com/iridiumblue/articles/blob/master/roc_star.md\n",
        "        _y_true: `Tensor`. Targets (labels).  Float either 0.0 or 1.0 .\n",
        "        y_pred: `Tensor` . Predictions.\n",
        "        gamma  : `Float` Gamma, as derived from last epoch.\n",
        "        _epoch_true: `Tensor`.  Targets (labels) from last epoch.\n",
        "        epoch_pred : `Tensor`.  Predicions from last epoch.\n",
        "    \"\"\"\n",
        "    #convert labels to boolean\n",
        "    y_true = (_y_true>=0.50)\n",
        "    epoch_true = (_epoch_true>=0.50)\n",
        "\n",
        "    # if batch is either all true or false return small random stub value.\n",
        "    if torch.sum(y_true)==0 or torch.sum(y_true) == y_true.shape[0]: return torch.sum(y_pred)*1e-8\n",
        "\n",
        "    pos = y_pred[y_true]\n",
        "    neg = y_pred[~y_true]\n",
        "\n",
        "    epoch_pos = epoch_pred[epoch_true]\n",
        "    epoch_neg = epoch_pred[~epoch_true]\n",
        "\n",
        "    # Take random subsamples of the training set, both positive and negative.\n",
        "    max_pos = 1000 # Max number of positive training samples\n",
        "    max_neg = 1000 # Max number of positive training samples\n",
        "    cap_pos = epoch_pos.shape[0]\n",
        "    cap_neg = epoch_neg.shape[0]\n",
        "    epoch_pos = epoch_pos[torch.rand_like(epoch_pos) < max_pos/cap_pos]\n",
        "    epoch_neg = epoch_neg[torch.rand_like(epoch_neg) < max_neg/cap_pos]\n",
        "\n",
        "    ln_pos = pos.shape[0]\n",
        "    ln_neg = neg.shape[0]\n",
        "\n",
        "    # sum positive batch elements agaionst (subsampled) negative elements\n",
        "    if ln_pos>0 :\n",
        "        pos_expand = pos.view(-1,1).expand(-1,epoch_neg.shape[0]).reshape(-1)\n",
        "        neg_expand = epoch_neg.repeat(ln_pos)\n",
        "\n",
        "        diff2 = neg_expand - pos_expand + gamma\n",
        "        l2 = diff2[diff2>0]\n",
        "        m2 = l2 * l2\n",
        "        len2 = l2.shape[0]\n",
        "    else:\n",
        "        m2 = torch.tensor([0], dtype=torch.float).cuda()\n",
        "        len2 = 0\n",
        "\n",
        "    # Similarly, compare negative batch elements against (subsampled) positive elements\n",
        "    if ln_neg>0 :\n",
        "        pos_expand = epoch_pos.view(-1,1).expand(-1, ln_neg).reshape(-1)\n",
        "        neg_expand = neg.repeat(epoch_pos.shape[0])\n",
        "\n",
        "        diff3 = neg_expand - pos_expand + gamma\n",
        "        l3 = diff3[diff3>0]\n",
        "        m3 = l3*l3\n",
        "        len3 = l3.shape[0]\n",
        "    else:\n",
        "        m3 = torch.tensor([0], dtype=torch.float).cuda()\n",
        "        len3=0\n",
        "\n",
        "    if (torch.sum(m2)+torch.sum(m3))!=0 :\n",
        "       res2 = torch.sum(m2)/max_pos+torch.sum(m3)/max_neg\n",
        "       #code.interact(local=dict(globals(), **locals()))\n",
        "    else:\n",
        "       res2 = torch.sum(m2)+torch.sum(m3)\n",
        "\n",
        "    res2 = torch.where(torch.isnan(res2), torch.zeros_like(res2), res2)\n",
        "\n",
        "    return res2"
      ],
      "metadata": {
        "id": "-Oko6cbcLgxp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_update_gamma(y_true,y_pred, epoch=-1,delta=2):\n",
        "    \"\"\"\n",
        "    Calculate gamma from last epoch's targets and predictions.\n",
        "    Gamma is updated at the end of each epoch.\n",
        "    y_true: `Tensor`. Targets (labels).  Float either 0.0 or 1.0 .\n",
        "    y_pred: `Tensor` . Predictions.\n",
        "    \"\"\"\n",
        "    DELTA = delta\n",
        "    SUB_SAMPLE_SIZE = 2000.0\n",
        "    pos = y_pred[y_true==1]\n",
        "    neg = y_pred[y_true==0] # yo pytorch, no boolean tensors or operators?  Wassap?\n",
        "    # subsample the training set for performance\n",
        "    cap_pos = pos.shape[0]\n",
        "    cap_neg = neg.shape[0]\n",
        "    pos = pos[torch.rand_like(pos) < SUB_SAMPLE_SIZE/cap_pos]\n",
        "    neg = neg[torch.rand_like(neg) < SUB_SAMPLE_SIZE/cap_neg]\n",
        "    ln_pos = pos.shape[0]\n",
        "    ln_neg = neg.shape[0]\n",
        "    pos_expand = pos.view(-1,1).expand(-1,ln_neg).reshape(-1)\n",
        "    neg_expand = neg.repeat(ln_pos)\n",
        "    diff = neg_expand - pos_expand\n",
        "    ln_All = diff.shape[0]\n",
        "    Lp = diff[diff>0] # because we're taking positive diffs, we got pos and neg flipped.\n",
        "    ln_Lp = Lp.shape[0]-1\n",
        "    diff_neg = -1.0 * diff[diff<0]\n",
        "    diff_neg = diff_neg.sort()[0]\n",
        "    ln_neg = diff_neg.shape[0]-1\n",
        "    ln_neg = max([ln_neg, 0])\n",
        "    left_wing = int(ln_Lp*DELTA)\n",
        "    left_wing = max([0,left_wing])\n",
        "    left_wing = min([ln_neg,left_wing])\n",
        "    default_gamma=torch.tensor(0.2, dtype=torch.float).cuda()\n",
        "    if diff_neg.shape[0] > 0 :\n",
        "       gamma = diff_neg[left_wing]\n",
        "    else:\n",
        "       gamma = default_gamma # default=torch.tensor(0.2, dtype=torch.float).cuda() #zoink\n",
        "    L1 = diff[diff>-1.0*gamma]\n",
        "    ln_L1 = L1.shape[0]\n",
        "    if epoch > -1 :\n",
        "        return gamma\n",
        "    else :\n",
        "        return default_gamma"
      ],
      "metadata": {
        "id": "8JoUhDY4Lzhi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rm2f9p8tHj9"
      },
      "source": [
        "## Attention model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9vy_IQ1JtHj-"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.functional import F\n",
        "\n",
        "class AttentionMIL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AttentionMIL, self).__init__()\n",
        "        self.L = 2048\n",
        "        self.D = 256\n",
        "        self.K = 16\n",
        "\n",
        "        self.feature_extractor_q = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.feature_extractor_k = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.attention_k = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, self.K)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.L, self.D),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.D, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        q = self.feature_extractor_q(x)  # BxNxK\n",
        "\n",
        "        k = self.feature_extractor_k(x) #BxNxK\n",
        "        kt = torch.transpose(k, 1, 2)  # BxKxN\n",
        "\n",
        "        A = F.softmax(q@kt, dim=1)  # BxNxN\n",
        "\n",
        "        M = A@x  # (BxNxN) (BxNxK) -> (BxNxK)\n",
        "\n",
        "        M = torch.mean(M, dim=1)\n",
        "\n",
        "        Y_prob = self.classifier(M)\n",
        "        return Y_prob.squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "N50Ej0oVtHj-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, df, features_dir, test = False):\n",
        "        self.df = df\n",
        "        self.features_dir = features_dir\n",
        "        self.test = test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.test:\n",
        "            sample = self.df.iloc[idx][\"Sample ID\"]\n",
        "        else:\n",
        "            sample, label = self.df.iloc[idx][[\"Sample ID\", \"Target\"]]\n",
        "            \n",
        "        # load the coordinates and features (1000, 3+2048)\n",
        "        _features = np.load(self.features_dir / sample)\n",
        "        # get coordinates (zoom level, tile x-coord on the slide, tile y-coord on the slide)\n",
        "        # and the MoCo V2 features\n",
        "        coordinates, features = _features[:, :3], _features[:, 3:]  # Ks\n",
        "\n",
        "        if self.test:\n",
        "            return {'x': features}\n",
        "        return {'x':features, 'y':label}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fnSank0stHj_"
      },
      "outputs": [],
      "source": [
        "N_split = 250\n",
        "train_dataset = MyDataset(df=df_train.iloc[:N_split], features_dir=train_features_dir)\n",
        "val_dataset = MyDataset(df=df_train.iloc[N_split:], features_dir=train_features_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lf2crsNxtHj_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "loader = DataLoader(dataset=train_dataset, batch_size=32)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "DE5Mo1zgtHkA"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCELoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "model = AttentionMIL().to(device)\n",
        "criterion = BCELoss().to(device)\n",
        "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, tolerance=5, min_delta=0):\n",
        "\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.last_best_val = 1e12\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, train_loss, validation_loss):\n",
        "        if validation_loss >= self.last_best_val:\n",
        "            self.counter +=1\n",
        "            if self.counter >= self.tolerance:  \n",
        "                self.early_stop = True\n",
        "        else :\n",
        "            self.counter = 0\n",
        "            self.last_best_val = validation_loss\n",
        "\n",
        "class SaveBestModel:\n",
        "    \"\"\"\n",
        "    Class to save the best model while training. If the current epoch's \n",
        "    validation loss is less than the previous least less, then save the\n",
        "    model state.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, best_valid_loss=float('inf'), save_path='outputs/best_model.pth'\n",
        "    ):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "        self.save_path=save_path\n",
        "        \n",
        "    def __call__(\n",
        "        self, current_valid_loss, \n",
        "        epoch, model, optimizer, criterion\n",
        "    ):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            print(f\"\\nBest validation loss: {self.best_valid_loss}\")\n",
        "            print(f\"\\nSaving best model for epoch: {epoch+1}\\n\")\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, self.save_path)"
      ],
      "metadata": {
        "id": "4r1XPnf96geY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "b-Pi_55RtHkA"
      },
      "outputs": [],
      "source": [
        "def train_fold(model, optimizer, criterion, loader, val_loader=[], n_epoch = 10, verbose=1, sigma=1e-2, model_name='model'):\n",
        "    early_stopping = EarlyStopping(tolerance = 10, min_delta=0)\n",
        "    save_best_model = SaveBestModel(save_path='./outputs_'+model_name+'_best_weights.pth')\n",
        "    \n",
        "    last_epoch_y_pred = torch.tensor( 1.0-np.random.rand(len(loader.dataset))/2.0, dtype=torch.float).to(device)\n",
        "\n",
        "    last_epoch_y_t    = torch.tensor(np.concatenate([o['y'].float() for o in loader]),dtype=torch.float).to(device)\n",
        "    epoch_gamma = 0.20\n",
        "    for epoch in range(n_epoch):\n",
        "        mean_loss = 0\n",
        "        epoch_y_pred=[]\n",
        "        epoch_y_t=[]\n",
        "        for i, val in enumerate(loader):\n",
        "            labels = val['y'].to(device)\n",
        "            x = val['x'].to(device)\n",
        "            \n",
        "            noise = sigma * torch.randn_like(x)\n",
        "            x = x + noise\n",
        "            #print(labels.shape)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            #print(output.shape)\n",
        "            loss = roc_star_loss(labels.float(), output, epoch_gamma, last_epoch_y_t, last_epoch_y_pred)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            mean_loss = 1/(i+1) * (loss - mean_loss) + i / (i+1) * mean_loss\n",
        "\n",
        "            epoch_y_pred.extend(output)\n",
        "            epoch_y_t.extend(labels.float())\n",
        "        \n",
        "\n",
        "        val_loss = 0\n",
        "        for i, val in enumerate(val_loader):\n",
        "            model.eval()\n",
        "\n",
        "            labels = val['y'].to(device)\n",
        "            x = val['x'].to(device)\n",
        "            \n",
        "            output = model(x)\n",
        "            \n",
        "            v_loss = criterion(output, labels.float())\n",
        "\n",
        "            val_loss = 1/(i+1) * (v_loss - val_loss) + i / (i+1) * val_loss\n",
        "        \n",
        "        last_epoch_y_pred = torch.tensor(epoch_y_pred).to(device)\n",
        "        last_epoch_y_t = torch.tensor(epoch_y_t).to(device)\n",
        "        epoch_gamma = epoch_update_gamma(last_epoch_y_t, last_epoch_y_pred, epoch)\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch} - loss {mean_loss:.4f} - val loss {val_loss:.4f}\")\n",
        "        \n",
        "        #best model\n",
        "        save_best_model(val_loss, epoch=epoch, model=model, optimizer=optimizer, criterion=criterion)\n",
        "        # early stopping\n",
        "        early_stopping(loss, val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "          print(\"We are at epoch:\", epoch)\n",
        "          del last_epoch_y_pred, last_epoch_y_t\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lWHCeuD6tHkB",
        "outputId": "d91accf0-256f-4af3-944f-cf8bf1db20ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - loss 0.1751 - val loss 0.3509\n",
            "\n",
            "Best validation loss: 0.3508898913860321\n",
            "\n",
            "Saving best model for epoch: 1\n",
            "\n",
            "Epoch 1 - loss 0.0001 - val loss 0.3498\n",
            "\n",
            "Best validation loss: 0.34976857900619507\n",
            "\n",
            "Saving best model for epoch: 2\n",
            "\n",
            "Epoch 2 - loss 0.0001 - val loss 0.3491\n",
            "\n",
            "Best validation loss: 0.34913206100463867\n",
            "\n",
            "Saving best model for epoch: 3\n",
            "\n",
            "Epoch 3 - loss 0.0000 - val loss 0.3489\n",
            "\n",
            "Best validation loss: 0.34888550639152527\n",
            "\n",
            "Saving best model for epoch: 4\n",
            "\n",
            "Epoch 4 - loss 0.0000 - val loss 0.3487\n",
            "\n",
            "Best validation loss: 0.34868738055229187\n",
            "\n",
            "Saving best model for epoch: 5\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-586de365ae7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-c41daf0bce47>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(model, optimizer, criterion, loader, val_loader, n_epoch, verbose, sigma, model_name)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m#print(output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_star_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch_y_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ce927c006f07>\u001b[0m in \u001b[0;36mroc_star_loss\u001b[0;34m(_y_true, y_pred, gamma, _epoch_true, epoch_pred)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# if batch is either all true or false return small random stub value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_fold(model, optimizer, criterion, loader, val_loader, n_epoch=200, verbose=1, sigma = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the best model checkpoint\n",
        "best_model_cp = torch.load('outputs_model_best_weights.pth')\n",
        "best_model_epoch = best_model_cp['epoch']\n",
        "print(f\"Best model was saved at {best_model_epoch} epochs\\n\")"
      ],
      "metadata": {
        "id": "OVoKhXab_Cw-",
        "outputId": "1a9396d2-39d2-4c2d-ac49-7f5be853ee73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model was saved at 5 epochs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_cp = torch.load('outputs_model_best_weights.pth')\n",
        "model.load_state_dict(best_model_cp['model_state_dict'])"
      ],
      "metadata": {
        "id": "niI4xrE8_l1F",
        "outputId": "588f5958-2002-4001-8ac1-df36d87c2658",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zwyob_intHkB",
        "outputId": "47363bd8-2c01-4893-fcb7-74054174c6bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4106800766283525\n"
          ]
        }
      ],
      "source": [
        "y_trues = []\n",
        "preds = []\n",
        "for val in val_loader:\n",
        "    model.eval()\n",
        "    y_true = val['y'].detach().numpy().tolist()\n",
        "    x = val['x'].to(device)\n",
        "\n",
        "    pred = model(x).detach().cpu().numpy().tolist()\n",
        "    y_trues.extend(y_true)\n",
        "    preds.extend(pred)\n",
        "\n",
        "auc = roc_auc_score(y_trues, preds)\n",
        "\n",
        "print(auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t4kjgMFtHkC"
      },
      "source": [
        "## 5-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:52.905566Z",
          "start_time": "2022-10-24T07:22:52.893435Z"
        },
        "id": "th4Cs0iqtHkC",
        "outputId": "bda2bf8a-176b-4bbd-a67f-1b3436ae6196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:3438: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set specifications\n",
            "---------------------------\n",
            "344 unique samples\n",
            "305 unique patients\n",
            "3 unique centers\n"
          ]
        }
      ],
      "source": [
        "# /!\\ we perform splits at the patient level so that all samples from the same patient\n",
        "# are found in the same split\n",
        "\n",
        "patients_unique = np.unique(patients_train)\n",
        "y_unique = np.array(\n",
        "    [np.mean(y_train[patients_train == p]) for p in patients_unique]\n",
        ")\n",
        "centers_unique = np.array(\n",
        "    [centers_train[patients_train == p][0] for p in patients_unique]\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Training set specifications\\n\"\n",
        "    \"---------------------------\\n\"\n",
        "    f\"{len(df_train)} unique samples\\n\"\n",
        "    f\"{len(patients_unique)} unique patients\\n\"\n",
        "    f\"{len(np.unique(centers_unique))} unique centers\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:54.077640Z",
          "start_time": "2022-10-24T07:22:52.907331Z"
        },
        "id": "9BWyq3AqtHkC",
        "outputId": "2523ff53-177a-4029-ac6b-25a4a3c3c469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 705
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best validation loss: 0.6068194508552551\n",
            "\n",
            "Saving best model for epoch: 1\n",
            "\n",
            "\n",
            "Best validation loss: 0.5158516764640808\n",
            "\n",
            "Saving best model for epoch: 2\n",
            "\n",
            "\n",
            "Best validation loss: 0.49128687381744385\n",
            "\n",
            "Saving best model for epoch: 3\n",
            "\n",
            "\n",
            "Best validation loss: 0.45554786920547485\n",
            "\n",
            "Saving best model for epoch: 6\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3495eb368852>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Attention_K{k}_F{fold}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtrain_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_fold_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader_fold_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbest_model_cp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'outputs_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_best_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-c41daf0bce47>\u001b[0m in \u001b[0;36mtrain_fold\u001b[0;34m(model, optimizer, criterion, loader, val_loader, n_epoch, verbose, sigma, model_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "aucs = []\n",
        "models = []\n",
        "# 5-fold CV is repeated 5 times with different random states\n",
        "for k in range(5):\n",
        "    kfold = StratifiedKFold(5, shuffle=True, random_state=k)\n",
        "    fold = 0\n",
        "    # split is performed at the patient-level\n",
        "    for train_idx_, val_idx_ in kfold.split(patients_unique, y_unique):\n",
        "        # retrieve the indexes of the samples corresponding to the\n",
        "        # patients in `train_idx_` and `test_idx_`\n",
        "        train_idx = np.arange(len(df_train))[\n",
        "            pd.Series(patients_train).isin(patients_unique[train_idx_])\n",
        "        ]\n",
        "        val_idx = np.arange(len(df_train))[\n",
        "            pd.Series(patients_train).isin(patients_unique[val_idx_])\n",
        "        ]\n",
        "        # set the training and validation folds\n",
        "        df_fold_train = df_train.iloc[train_idx]\n",
        "        data_fold_train = MyDataset(df_fold_train, train_features_dir)\n",
        "        loader_fold_train = DataLoader(data_fold_train, batch_size=40, shuffle=True)\n",
        "\n",
        "        df_fold_val = df_train.iloc[val_idx]\n",
        "        data_fold_val = MyDataset(df_fold_val, train_features_dir)\n",
        "        loader_fold_val = DataLoader(data_fold_val, batch_size=40, shuffle=True)\n",
        "        \n",
        "        # instantiate the model\n",
        "        model = AttentionMIL().to(device)\n",
        "        criterion = BCELoss().to(device)\n",
        "        optimizer = Adam(model.parameters(), lr=0.0005, weight_decay=1e-3)\n",
        "        \n",
        "        model_name = f'Attention_K{k}_F{fold}'\n",
        "        train_fold(model, optimizer, criterion, loader_fold_train, val_loader=loader_fold_val, n_epoch=200, verbose=0, sigma = 0.001, model_name=model_name)\n",
        "\n",
        "        best_model_cp = torch.load('outputs_'+ model_name + '_best_weights.pth')\n",
        "        model.load_state_dict(best_model_cp['model_state_dict'])\n",
        "        \n",
        "        # get the predictions (1-d probability)\n",
        "        y_trues = []\n",
        "        preds = []\n",
        "        for val in loader_fold_val:\n",
        "            model.eval()\n",
        "            y_true = val['y'].detach().cpu().numpy().tolist()\n",
        "            x = val['x'].to(device)\n",
        "\n",
        "            pred = model(x).detach().cpu().numpy().tolist()\n",
        "            y_trues.extend(y_true)\n",
        "            preds.extend(pred)\n",
        "\n",
        "        auc = roc_auc_score(y_trues, preds)\n",
        "\n",
        "        print(f\"AUC on split {k} fold {fold}: {auc:.3f}\")\n",
        "        aucs.append(auc)\n",
        "        # add the logistic regression to the list of classifiers\n",
        "        models.append(model_name)\n",
        "\n",
        "        del model\n",
        "        fold += 1\n",
        "    print(\"----------------------------\")\n",
        "print(\n",
        "    f\"5-fold cross-validated AUC averaged over {k+1} repeats: \"\n",
        "    f\"{np.mean(aucs):.3f} ({np.std(aucs):.3f})\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5QBiOMltHkD"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXszLbxWtHkD"
      },
      "source": [
        "Now we evaluate the previous models trained through cross-validation so that to produce a submission file that can directly be uploaded on the data challenge platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WLw6VpwtHkD"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "T97MdixbxVfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:54.987815Z",
          "start_time": "2022-10-24T07:22:54.079916Z"
        },
        "id": "SZPmMi50tHkD"
      },
      "outputs": [],
      "source": [
        "dataset_test = MyDataset(df_test, test_features_dir, test=True)\n",
        "\n",
        "loader_test = DataLoader(dataset_test, batch_size=20, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test[1]"
      ],
      "metadata": {
        "id": "Xi_VmNkyw44D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-20T08:17:35.617554Z",
          "start_time": "2022-10-20T08:17:35.603562Z"
        },
        "id": "O9GbDCC4tHkE"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:55.043255Z",
          "start_time": "2022-10-24T07:22:54.989274Z"
        },
        "id": "6i0le2B2tHkE"
      },
      "outputs": [],
      "source": [
        "preds_test = 0\n",
        "\n",
        "\n",
        "# loop over the classifiers\n",
        "for model_name in models:\n",
        "    model = AttentionMIL().to(device)\n",
        "    best_model_cp = torch.load('outputs_'+ model_name + '_best_weights.pth')\n",
        "    model.load_state_dict(best_model_cp['model_state_dict'])\n",
        "    \n",
        "    preds = []\n",
        "    for val in loader_test:\n",
        "        model.eval()\n",
        "        x = val['x'].to(device)\n",
        "\n",
        "        pred = model(x).detach().cpu().numpy().tolist()\n",
        "        preds.extend(pred)\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    preds_test += preds\n",
        "# and take the average (ensembling technique)\n",
        "preds_test = preds_test / len(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Ax63WHtHkE"
      },
      "source": [
        "## Saving predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:22:55.098571Z",
          "start_time": "2022-10-24T07:22:55.044975Z"
        },
        "id": "of-SNwlhtHkF"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame(\n",
        "    {\"Sample ID\": df_test[\"Sample ID\"].values, \"Target\": preds_test}\n",
        ").sort_values(\n",
        "    \"Sample ID\"\n",
        ")  # extra step to sort the sample IDs\n",
        "\n",
        "# sanity checks\n",
        "assert all(submission[\"Target\"].between(0, 1)), \"`Target` values must be in [0, 1]\"\n",
        "assert submission.shape == (149, 2), \"Your submission file must be of shape (149, 2)\"\n",
        "assert list(submission.columns) == [\n",
        "    \"Sample ID\",\n",
        "    \"Target\",\n",
        "], \"Your submission file must have columns `Sample ID` and `Target`\"\n",
        "\n",
        "# save the submission as a csv file\n",
        "submission.to_csv(data_dir / \"benchmark_test_output.csv\", index=None)\n",
        "submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZDlA_QtHkF"
      },
      "source": [
        "# Dealing with images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4N09eRLtHkF"
      },
      "source": [
        "The following code aims to load and manipulate the images provided as part of  this challenge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo0uzTl-tHkG"
      },
      "source": [
        "## Scanning images paths on disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhkZ3RDftHkG"
      },
      "source": [
        "This operation can take up to 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:23:00.263580Z",
          "start_time": "2022-10-24T07:22:55.100342Z"
        },
        "id": "nRB1V26ktHkG"
      },
      "outputs": [],
      "source": [
        "train_images_dir = data_dir / \"train_input\" / \"images\"\n",
        "train_images_files = list(train_images_dir.rglob(\"*.jpg\"))\n",
        "\n",
        "test_images_dir = data_dir / \"test_input\" / \"images\"\n",
        "test_images_files = list(test_images_dir.rglob(\"*.jpg\"))\n",
        "\n",
        "print(\n",
        "    f\"Number of images\\n\"\n",
        "    \"-----------------\\n\"\n",
        "    f\"Train: {len(train_images_files)}\\n\" # 344 x 1000 = 344,000 tiles\n",
        "    f\"Test: {len(test_images_files)}\\n\"  # 149 x 1000 = 149,000 tiles\n",
        "    f\"Total: {len(train_images_files) + len(test_images_files)}\\n\"  # 493 x 1000 = 493,000 tiles\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-20T10:16:48.078600Z",
          "start_time": "2022-10-20T10:16:47.948127Z"
        },
        "id": "nHmmEQx4tHkH"
      },
      "source": [
        "## Reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vULU7SitHkH"
      },
      "source": [
        "Now we can load some of the `.jpg` images for a given sample, say `ID_001`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:23:00.381225Z",
          "start_time": "2022-10-24T07:23:00.267047Z"
        },
        "id": "rLyRuEe8tHkI"
      },
      "outputs": [],
      "source": [
        "ID_001_tiles = [p for p in train_images_files if 'ID_001' in p.name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:23:01.973155Z",
          "start_time": "2022-10-24T07:23:00.382760Z"
        },
        "id": "P4E6K-mMtHkI"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(5, 5)\n",
        "fig.set_size_inches(12, 12)\n",
        "\n",
        "for i, img_file in enumerate(ID_001_tiles[:25]):\n",
        "    # get the metadata from the file path\n",
        "    _, metadata = str(img_file).split(\"tile_\")\n",
        "    id_tile, level, x, y = metadata[:-4].split(\"_\")\n",
        "    img = plt.imread(img_file)\n",
        "    ax = axes[i//5, i%5]\n",
        "    ax.imshow(img)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"Tile {id_tile} ({x}, {y})\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iEVWY8otHkI"
      },
      "source": [
        "## Mapping with features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM6yjhS1tHkJ"
      },
      "source": [
        "Note that the coordinates in the features matrices and tiles number are aligned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:23:01.984327Z",
          "start_time": "2022-10-24T07:23:01.974933Z"
        },
        "id": "iAoTnjxLtHkJ"
      },
      "outputs": [],
      "source": [
        "sample = \"ID_001.npy\"\n",
        "_features = np.load(train_features_dir / sample)\n",
        "coordinates, features = _features[:, :3], _features[:, 3:]\n",
        "print(\"xy features coordinates\")\n",
        "coordinates[:10, 1:].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-10-24T07:23:01.990342Z",
          "start_time": "2022-10-24T07:23:01.985926Z"
        },
        "id": "IrAp_y9dtHkK"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    \"Tiles numbering and features coordinates\\n\"\n",
        ")\n",
        "[tile.name for tile in ID_001_tiles[:10]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP_5IxWNtHkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHgk7DOxtHkK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}