{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, you will need to install some packages to reproduce the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:49.282534Z",
     "start_time": "2022-10-24T07:22:35.359325Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:50.448803Z",
     "start_time": "2022-10-24T07:22:49.285830Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "PATH_COLAB = '/content/drive/MyDrive/challenge_ens_2023_small/moco_features.zip'\n",
    "PATH_DEVICE = '..'\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    logging.info('Working on Colab.')\n",
    "    \n",
    "    # connect your drive to the session\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    %cd /content/drive/MyDrive/challenge_data_ens_small/\n",
    "\n",
    "    # unzip data into the colab session\n",
    "    ! unzip $PATH_COLAB -d /content\n",
    "    logging.info('Data unziped in your Drive.')\n",
    "\n",
    "    %cd /content\n",
    "\n",
    "    %cp -R drive/MyDrive/challenge_ens_2023_small/supplementary_data/ .\n",
    "    %cp drive/MyDrive/challenge_ens_2023_small/train_output.csv .\n",
    "\n",
    "\n",
    "except:\n",
    "    logging.info('Working on your device.')\n",
    "    \n",
    "    data_exists = os.path.exists(PATH_DEVICE + '/train_input') and os.path.exists(PATH_DEVICE + '/test_input') and os.path.exists(PATH_DEVICE + '/train_output.csv')\n",
    "    \n",
    "    if data_exists:\n",
    "        logging.info(f\"Dataset found on device at : '{PATH_DEVICE}.'\") \n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Data folder not found at '{PATH_DEVICE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T15:29:41.479814Z",
     "start_time": "2022-10-20T15:29:41.472869Z"
    }
   },
   "source": [
    "After downloading or unzipping the downloaded files, your data tree must have the following architecture in order to properly run the notebook:\n",
    "```\n",
    "your_data_dir/\n",
    "├── train_output.csv\n",
    "├── train_input/\n",
    "│   ├── images/\n",
    "│       ├── ID_001/\n",
    "│           ├── ID_001_tile_000_17_170_43.jpg\n",
    "...\n",
    "│   └── moco_features/\n",
    "│       ├── ID_001.npy\n",
    "...\n",
    "├── test_input/\n",
    "│   ├── images/\n",
    "│       ├── ID_003/\n",
    "│           ├── ID_003_tile_000_16_114_93.jpg\n",
    "...\n",
    "│   └── moco_features/\n",
    "│       ├── ID_003.npy\n",
    "...\n",
    "├── supplementary_data/\n",
    "│   ├── baseline.ipynb\n",
    "│   ├── test_metadata.csv\n",
    "│   └── train_metadata.csv\n",
    "```\n",
    "\n",
    "For instance, `your_data_dir = /storage/DATA_CHALLENGE_ENS_2022/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to reproduce the baseline method on this challenge called `MeanPool`. This method consists in a logistic regression learnt on top of tile-level MoCo V2 features averaged over the slides.\n",
    "\n",
    "For a given slide $s$ with $N_s=1000$ tiles and corresponding MoCo V2 features $\\mathbf{K_s} \\in \\mathbb{R}^{(1000,\\,2048)}$, a slide-level average is performed over the tile axis.\n",
    "\n",
    "For $j=1,...,2048$:\n",
    "\n",
    "$$\\overline{\\mathbf{k_s}}(j) = \\frac{1}{N_s} \\sum_{i=1}^{N_s} \\mathbf{K_s}(i, j) $$\n",
    "\n",
    "Thus, the training input data for MeanPool consists of $S_{\\text{train}}=344$ mean feature vectors $\\mathbf{k_s}$, $s=1,...,S_{\\text{train}}$, where $S_{\\text{train}}$ denotes the number of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T09:25:58.896288Z",
     "start_time": "2022-10-20T09:25:58.161711Z"
    }
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:50.479040Z",
     "start_time": "2022-10-24T07:22:50.450662Z"
    }
   },
   "outputs": [],
   "source": [
    "# put your own path to the data root directory (see example in `Data architecture` section)\n",
    "data_dir = Path(\"../\")\n",
    "\n",
    "# load the training and testing data sets\n",
    "train_features_dir = data_dir / \"train_input\" / \"moco_features\"\n",
    "test_features_dir = data_dir / \"test_input\" / \"moco_features\"\n",
    "df_train = pd.read_csv(data_dir  / \"supplementary_data\" / \"train_metadata.csv\")\n",
    "df_test = pd.read_csv(data_dir  / \"supplementary_data\" / \"test_metadata.csv\")\n",
    "\n",
    "# concatenate y_train and df_train\n",
    "y_train = pd.read_csv(data_dir  / \"train_output.csv\")\n",
    "df_train = df_train.merge(y_train, on=\"Sample ID\")\n",
    "\n",
    "print(f\"Training data dimensions: {df_train.shape}\")  # (344, 4)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the features matrices $\\mathbf{K_s} \\in \\mathbb{R}^{(1000,\\,2048)}$ for $s=1,...,344$ and perform slide-level averaging. This operation should take at most 5 minutes on your laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:52.890700Z",
     "start_time": "2022-10-24T07:22:50.480795Z"
    }
   },
   "outputs": [],
   "source": [
    "#size_train = 50 #len(df_train)\n",
    "#X_train = np.zeros((size_train, 1000, 2048))\n",
    "#y_train = np.zeros((size_train), dtype=int)\n",
    "centers_train = []\n",
    "patients_train = []\n",
    "\n",
    "for i, (sample, label, center, patient) in enumerate(tqdm(\n",
    "    df_train[[\"Sample ID\", \"Target\", \"Center ID\", \"Patient ID\"]].values\n",
    ")):\n",
    "    # load the coordinates and features (1000, 3+2048)\n",
    "    #_features = np.load(train_features_dir / sample)\n",
    "    # get coordinates (zoom level, tile x-coord on the slide, tile y-coord on the slide)\n",
    "    # and the MoCo V2 features\n",
    "    #coordinates, features = _features[:, :3], _features[:, 3:]  # Ks\n",
    "    # slide-level averaging\n",
    "    #X_train[i] = features\n",
    "    #y_train[i] = label\n",
    "    centers_train.append(center)\n",
    "    patients_train.append(patient)\n",
    "    \n",
    "\n",
    "# convert to numpy arrays\n",
    "#X_train = np.array(X_train)\n",
    "#y_train = np.array(y_train)\n",
    "centers_train = np.array(centers_train)\n",
    "patients_train = np.array(patients_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer perceptron with a max over an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.functional import F\n",
    "\n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        \n",
    "        self.input = nn.Linear(2048,512)\n",
    "\n",
    "        self.hidden = nn.Linear(512, 1)\n",
    "\n",
    "        #self.output = nn.Linear(128, 1)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p = 0.3)\n",
    "\n",
    "        self.relu = torch.nn.ReLU() # instead of Heaviside step fn  \n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        self.max = torch.nn  \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        #x = self.output(x)\n",
    "        #x = self.sigmoid(x)\n",
    "\n",
    "        x = x.squeeze()\n",
    "        output = torch.max(x, dim=1).values # instead of Heaviside step fn\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, features_dir, test = False):\n",
    "        self.df_train = df\n",
    "        self.features_dir = features_dir\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.test:\n",
    "            sample = df_train.iloc[idx][[\"Sample ID\"]]\n",
    "        else:\n",
    "            sample, label =df_train.iloc[idx][[\"Sample ID\", \"Target\"]]\n",
    "            \n",
    "        # load the coordinates and features (1000, 3+2048)\n",
    "        _features = np.load(self.features_dir / sample)\n",
    "        # get coordinates (zoom level, tile x-coord on the slide, tile y-coord on the slide)\n",
    "        # and the MoCo V2 features\n",
    "        coordinates, features = _features[:, :3], _features[:, 3:]  # Ks\n",
    "\n",
    "        if self.test:\n",
    "            return {'x': features}\n",
    "        return {'x':features, 'y':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_split = 150\n",
    "train_dataset = MyDataset(df=df_train.iloc[:N_split], features_dir=train_features_dir)\n",
    "val_dataset = MyDataset(df=df_train.iloc[N_split:], features_dir=train_features_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "loader = DataLoader(dataset=train_dataset, batch_size=43)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=43)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCELoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "model = Perceptron().to(device)\n",
    "criterion = BCELoss().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    mean_loss = 0\n",
    "    for i, val in enumerate(loader):\n",
    "        labels = val['y'].to(device)\n",
    "        x = val['x'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        loss = criterion(output, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        mean_loss = 1/(i+1) * (loss - mean_loss) + i / (i+1) * mean_loss\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fold(model, loader, val_loader=[], n_epoch = 10, verbose=1):\n",
    "    for epoch in range(10):\n",
    "        loss = train(loader)\n",
    "\n",
    "        val_loss = 0\n",
    "        for i, val in enumerate(val_loader):\n",
    "            model.eval()\n",
    "\n",
    "            labels = val['y'].to(device)\n",
    "            x = val['x'].to(device)\n",
    "            \n",
    "            output = model(x)\n",
    "            \n",
    "            v_loss = criterion(output, labels.float())\n",
    "\n",
    "            val_loss = 1/(i+1) * (v_loss - val_loss) + i / (i+1) * val_loss\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch} - loss {loss:.4f} - val loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold(model, loader, n_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trues = []\n",
    "preds = []\n",
    "for val in loader:\n",
    "    model.eval()\n",
    "    y_true = val['y'].detach().numpy().tolist()\n",
    "    x = val['x'].to(device)\n",
    "\n",
    "    pred = model(x).detach().numpy().tolist()\n",
    "    y_trues.extend(y_true)\n",
    "    preds.extend(pred)\n",
    "\n",
    "auc = roc_auc_score(y_trues, preds)\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:52.905566Z",
     "start_time": "2022-10-24T07:22:52.893435Z"
    }
   },
   "outputs": [],
   "source": [
    "# /!\\ we perform splits at the patient level so that all samples from the same patient\n",
    "# are found in the same split\n",
    "\n",
    "patients_unique = np.unique(patients_train)\n",
    "y_unique = np.array(\n",
    "    [np.mean(y_train[patients_train == p]) for p in patients_unique]\n",
    ")\n",
    "centers_unique = np.array(\n",
    "    [centers_train[patients_train == p][0] for p in patients_unique]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Training set specifications\\n\"\n",
    "    \"---------------------------\\n\"\n",
    "    f\"{len(df_train)} unique samples\\n\"\n",
    "    f\"{len(patients_unique)} unique patients\\n\"\n",
    "    f\"{len(np.unique(centers_unique))} unique centers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:54.077640Z",
     "start_time": "2022-10-24T07:22:52.907331Z"
    }
   },
   "outputs": [],
   "source": [
    "aucs = []\n",
    "models = []\n",
    "# 5-fold CV is repeated 5 times with different random states\n",
    "for k in range(5):\n",
    "    kfold = StratifiedKFold(5, shuffle=True, random_state=k)\n",
    "    fold = 0\n",
    "    # split is performed at the patient-level\n",
    "    for train_idx_, val_idx_ in kfold.split(patients_unique, y_unique):\n",
    "        # retrieve the indexes of the samples corresponding to the\n",
    "        # patients in `train_idx_` and `test_idx_`\n",
    "        train_idx = np.arange(len(df_train))[\n",
    "            pd.Series(patients_train).isin(patients_unique[train_idx_])\n",
    "        ]\n",
    "        val_idx = np.arange(len(df_train))[\n",
    "            pd.Series(patients_train).isin(patients_unique[val_idx_])\n",
    "        ]\n",
    "        # set the training and validation folds\n",
    "        df_fold_train = df_train.iloc[train_idx]\n",
    "        data_fold_train = MyDataset(df_fold_train, train_features_dir)\n",
    "        loader_fold_train = DataLoader(data_fold_train, batch_size=40, shuffle=True)\n",
    "\n",
    "        df_fold_val = df_train.iloc[val_idx]\n",
    "        data_fold_val = MyDataset(df_fold_val, train_features_dir)\n",
    "        loader_fold_val = DataLoader(data_fold_val, batch_size=40, shuffle=True)\n",
    "        \n",
    "        # instantiate the model\n",
    "        model = Perceptron().to(device)\n",
    "        criterion = BCELoss().to(device)\n",
    "        optimizer = Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        train_fold(model, loader_fold_train, n_epoch=20)\n",
    "\n",
    "        # get the predictions (1-d probability)\n",
    "        y_trues = []\n",
    "        preds = []\n",
    "        for val in loader_fold_val:\n",
    "            model.eval()\n",
    "            y_true = val['y'].detach().numpy().tolist()\n",
    "            x = val['x'].to(device)\n",
    "\n",
    "            pred = model(x).detach().numpy().tolist()\n",
    "            y_trues.extend(y_true)\n",
    "            preds.extend(pred)\n",
    "\n",
    "        auc = roc_auc_score(y_trues, preds)\n",
    "\n",
    "        print(f\"AUC on split {k} fold {fold}: {auc:.3f}\")\n",
    "        aucs.append(auc)\n",
    "        # add the logistic regression to the list of classifiers\n",
    "        models.append(model)\n",
    "        fold += 1\n",
    "    print(\"----------------------------\")\n",
    "print(\n",
    "    f\"5-fold cross-validated AUC averaged over {k+1} repeats: \"\n",
    "    f\"{np.mean(aucs):.3f} ({np.std(aucs):.3f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the previous models trained through cross-validation so that to produce a submission file that can directly be uploaded on the data challenge platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:54.987815Z",
     "start_time": "2022-10-24T07:22:54.079916Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_test = MyDataset(df_test, test_features_dir, test=True)\n",
    "\n",
    "loader_test = DataLoader(dataset_test, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T08:17:35.617554Z",
     "start_time": "2022-10-20T08:17:35.603562Z"
    }
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:55.043255Z",
     "start_time": "2022-10-24T07:22:54.989274Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_test = 0\n",
    "\n",
    "\n",
    "# loop over the classifiers\n",
    "for lr in models:\n",
    "    \n",
    "    preds = []\n",
    "    for val in loader_test:\n",
    "        model.eval()\n",
    "        x = val['x'].to(device)\n",
    "\n",
    "        pred = model(x).detach().numpy().tolist()\n",
    "        preds.extend(pred)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    preds_test += preds\n",
    "# and take the average (ensembling technique)\n",
    "preds_test = preds_test / len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:55.098571Z",
     "start_time": "2022-10-24T07:22:55.044975Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(\n",
    "    {\"Sample ID\": df_test[\"Sample ID\"].values, \"Target\": preds_test}\n",
    ").sort_values(\n",
    "    \"Sample ID\"\n",
    ")  # extra step to sort the sample IDs\n",
    "\n",
    "# sanity checks\n",
    "assert all(submission[\"Target\"].between(0, 1)), \"`Target` values must be in [0, 1]\"\n",
    "assert submission.shape == (149, 2), \"Your submission file must be of shape (149, 2)\"\n",
    "assert list(submission.columns) == [\n",
    "    \"Sample ID\",\n",
    "    \"Target\",\n",
    "], \"Your submission file must have columns `Sample ID` and `Target`\"\n",
    "\n",
    "# save the submission as a csv file\n",
    "submission.to_csv(data_dir / \"benchmark_test_output.csv\", index=None)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code aims to load and manipulate the images provided as part of  this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning images paths on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:00.263580Z",
     "start_time": "2022-10-24T07:22:55.100342Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images_dir = data_dir / \"train_input\" / \"images\"\n",
    "train_images_files = list(train_images_dir.rglob(\"*.jpg\"))\n",
    "\n",
    "test_images_dir = data_dir / \"test_input\" / \"images\"\n",
    "test_images_files = list(test_images_dir.rglob(\"*.jpg\"))\n",
    "\n",
    "print(\n",
    "    f\"Number of images\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Train: {len(train_images_files)}\\n\" # 344 x 1000 = 344,000 tiles\n",
    "    f\"Test: {len(test_images_files)}\\n\"  # 149 x 1000 = 149,000 tiles\n",
    "    f\"Total: {len(train_images_files) + len(test_images_files)}\\n\"  # 493 x 1000 = 493,000 tiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T10:16:48.078600Z",
     "start_time": "2022-10-20T10:16:47.948127Z"
    }
   },
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load some of the `.jpg` images for a given sample, say `ID_001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:00.381225Z",
     "start_time": "2022-10-24T07:23:00.267047Z"
    }
   },
   "outputs": [],
   "source": [
    "ID_001_tiles = [p for p in train_images_files if 'ID_001' in p.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:01.973155Z",
     "start_time": "2022-10-24T07:23:00.382760Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5)\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "for i, img_file in enumerate(ID_001_tiles[:25]):\n",
    "    # get the metadata from the file path\n",
    "    _, metadata = str(img_file).split(\"tile_\")\n",
    "    id_tile, level, x, y = metadata[:-4].split(\"_\")\n",
    "    img = plt.imread(img_file)\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(img)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"Tile {id_tile} ({x}, {y})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping with features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coordinates in the features matrices and tiles number are aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:01.984327Z",
     "start_time": "2022-10-24T07:23:01.974933Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = \"ID_001.npy\"\n",
    "_features = np.load(train_features_dir / sample)\n",
    "coordinates, features = _features[:, :3], _features[:, 3:]\n",
    "print(\"xy features coordinates\")\n",
    "coordinates[:10, 1:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:01.990342Z",
     "start_time": "2022-10-24T07:23:01.985926Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Tiles numbering and features coordinates\\n\"\n",
    ")\n",
    "[tile.name for tile in ID_001_tiles[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
